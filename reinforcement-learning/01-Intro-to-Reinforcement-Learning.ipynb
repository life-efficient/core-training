{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Reinforcement Learning\n",
    "\n",
    "### Prerequisites\n",
    "- [Basic Python]()\n",
    "- [Optimisation]()\n",
    "- [Logistic regression]() (for understanding softmax function)\n",
    "\n",
    "![](images/agent-env-loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is reinforcement learning?\n",
    "\n",
    "Reinforcement learning (RL) is way to train machines to perform tasks by rewarding them when they do well.\n",
    "\n",
    "We affect our environment by taking actions. Then we observe the new state of the environment and may or may not recieve some reward (or punishment) from the environment for taking that action. \n",
    "\n",
    "### Why reinforcement learning?\n",
    "\n",
    "This is how we, as humans, learn to do almost everything we know: \n",
    "\n",
    "Reinforcement learning is the mathematical formulation for how this learning occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setup\n",
    "\n",
    "### Can we train a reinforcement learning agent to balance a pole on top of a cart?\n",
    "![](images/cartpole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "To start understanding RL, it's useful to define a few things...\n",
    "\n",
    "### Agent\n",
    "An agent is something can interact with its enviroment to affect it - is has *agency* over its environment. E.g. a robot, a character in a video game or a computer system controlling traffic light signals in a city.\n",
    "\n",
    "### Environment\n",
    "The environment is what the agent can interact with. For a robot that might be a real environment like a room full of objects. It may be a simulation or a digital enviroment. Or it could be something more abstract like a set of traffic lights.\n",
    "\n",
    "### State\n",
    "\n",
    "At any point in time, the environment has a ***state*** which defines everything in the environment (perhaps as well as the agent's own state). This could include everything from all object positions, to the temperature, to information about other agents in the environment.\n",
    "\n",
    "At time $t$, a state $s_t$ represented completely by $k$ variables is a k-vector;\n",
    "$s_t = \\begin{bmatrix}s^1_t \\\\ \\vdots \\\\ s^k_t\\end{bmatrix}$\n",
    "\n",
    "### Observation\n",
    "An agent may not be able to know (or even sense) the whole state. In this case, what it can directly observenis called an **observation**.\n",
    "\n",
    "E.g. We as humans do not know the state of our entire environment (the universe). We can only observe a small part of it at a time.\n",
    "\n",
    "E.g. A robot might be able to perform some task better if it could hear. But if it is not equipped with a microphone, then these useful sounds would be missing from its information about the state.\n",
    "\n",
    "In the case where the agent cannot know the complete state, but only an observation, the problem is known as **partially observable**.\n",
    "\n",
    "At time $t$, an observation $o_t$ represented by $k$ variables (where k < number of variables that represent complete state) is a k-vector;\n",
    "$o_t = \\begin{bmatrix}o^1_t \\\\ \\vdots \\\\ o^k_t\\end{bmatrix}$\n",
    "\n",
    "### Actions\n",
    "Actions are things that the agent can choose to do to affect the environment. The set of all possible actions is called the **action space**.\n",
    "\n",
    "\n",
    "### Policy\n",
    "You probably know that in government, politicians have to follow certain *policies* that determine how they should respond to a situation. \n",
    "\n",
    "In RL, a **policy** means almost exactly the same thing: a policy defines what an agent should do when it finds itself in a certain state.\n",
    "\n",
    "Mathematically, a policy is a distribution over possible actions $a_t$, conditioned on the state $s_t$, at time $t$. We use the symbol $\\pi$ to represent the policy: $\\pi(a_t|s_t)$\n",
    "\n",
    "### Reward\n",
    "In RL, we train our agents by giving them *rewards* when they do the right thing. These rewards are not *actual* rewards like a bar of gold or a treat. Rather they are just what we try to make our agents get the maximum amount of by writing such code. Later we will discuss how to define and codify rewards for our agents.\n",
    "\n",
    "Reward for taking action $a_t$ in state $s_t$ at time $t$ is denoted as $r(s_t, a_t)$.\n",
    "\n",
    "### Trajectory\n",
    "A list of states that the agent encounters and the corresponding actions that it takes over one episode/lifetime/game.\n",
    "A trajectory until time T is denoted as $\\tau = (s_1, a_1, ... , s_T, a_T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In our case...\n",
    "\n",
    "**Agent** = the cart\n",
    "\n",
    "**Environment** = consists of the cart, the pole and the floor\n",
    "\n",
    "**State** = position of cart, velocity of cart, angle of pole, rotation rate of pole\n",
    "\n",
    "**Observation** = the state, because the environment is fully observable\n",
    "\n",
    "**Actions** = either push the cart right (action choice = 1) or push it left (action choice = 0)\n",
    "\n",
    "**Policy** = yet to be made, but will be a function that maps our observations to a distribution over the action space\n",
    "\n",
    "**Reward** = this gym environment returns +1 reward every timestep that the pole is still standing upright\n",
    "\n",
    "**Trajectory** = ([position of cart, velocity of cart, angle of pole, rotation rate of pole]@t=1, push left or right @t=1, ..., [position of cart, velocity of cart, angle of pole, rotation rate of pole]@t=T, push left or right @t=T)\n",
    "\n",
    "The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AI Gym\n",
    "\n",
    "Open AI gym provides a standardised set of environments that we can use to train reinforcement learning agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "Collecting pyglet<=1.3.2,>=1.2.0 (from gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl\n",
      "Collecting six (from gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting cloudpickle~=1.2.0 (from gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
      "Collecting scipy (from gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/29/50/a552a5aff252ae915f522e44642bb49a7b7b31677f9580cfd11bcc869976/scipy-1.3.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.10.4 (from gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/0e/46/ae6773894f7eacf53308086287897ec568eac9768918d913d5b9d366c5db/numpy-1.17.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting future (from pyglet<=1.3.2,>=1.2.0->gym)\n",
      "Installing collected packages: future, pyglet, six, cloudpickle, numpy, scipy, gym\n",
      "Successfully installed cloudpickle-1.2.2 future-0.18.2 gym-0.15.3 numpy-1.17.3 pyglet-1.3.2 scipy-1.3.1 six-1.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym       # bash command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00051033 -0.02143847  0.0197286  -0.04010987]\n",
      "0\n",
      "Observation: [-0.0009391   0.1733951   0.01892641 -0.32650352] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.0025288   0.36824255  0.01239634 -0.61315831] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 9.89365278e-03  5.63189090e-01  1.33169726e-04 -9.01911227e-01] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.02115743  0.75830924 -0.01790505 -1.19455229] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.03632362  0.95365841 -0.0417961  -1.49279289] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.05539679  0.7590692  -0.07165196 -1.21344845] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.07057817  0.95503889 -0.09592093 -1.5276964 ] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.08967895  1.15117835 -0.12647486 -1.84871067] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.11270252  1.34744577 -0.16344907 -2.17784489] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.13965143  1.15424774 -0.20700597 -1.93975062] \tReward: 1.0 \tDone? False \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.16273639  0.96185068 -0.24580098 -1.71774239] \tReward: 1.0 \tDone? True \tInfo: {} \tPrevious action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ice/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [ 0.1819734   0.77024326 -0.28015583 -1.51051004] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.19737827  0.57938442 -0.31036603 -1.31667607] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.20896595  0.7768131  -0.33669955 -1.68845965] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.22450222  0.58676208 -0.37046874 -1.51652   ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.23623746  0.39748332 -0.40079914 -1.35830691] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.24418712  0.20891137 -0.42796528 -1.21257097] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.24836535  0.40636467 -0.4522167  -1.60405508] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.25649264  0.60327547 -0.4842978  -1.99819767] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.26855815  0.41523183 -0.52426175 -1.88545172] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.27686279  0.22802101 -0.56197079 -1.78951965] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.28142321  0.04157448 -0.59776118 -1.70952056] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.2822547  -0.14417549 -0.63195159 -1.64467105] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.27937119 -0.32929402 -0.66484501 -1.5942913 ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.27278531 -0.5138429  -0.69673084 -1.55780744] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.26250845 -0.69788056 -0.72788699 -1.5347514 ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.24855084 -0.88146244 -0.75858202 -1.52475937] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.23092159 -0.68745759 -0.7890772  -1.93821652] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.21717244 -0.87123699 -0.82784153 -1.9526603 ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.1997477  -1.05467354 -0.86689474 -1.9830502 ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.17865423 -0.8635285  -0.90655575 -2.39273584] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.16138366 -0.67442193 -0.95441046 -2.79909242] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.14789522 -0.48786104 -1.01039231 -3.20076195] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.138138   -0.30438077 -1.07440755 -3.59607932] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.13205038 -0.12452207 -1.14632914 -3.98308336] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.12955994  0.05120063 -1.2259908  -4.35954651] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.13058396 -0.14414901 -1.31318173 -4.53719576] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.12770098  0.02360687 -1.40392565 -4.8856038 ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.12817311 -0.17779912 -1.50163772 -5.12534046] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.12461713 -0.38258664 -1.60414453 -5.39741036] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.1169654  -0.22767099 -1.71209274 -5.68349909] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.11241198 -0.4407134  -1.82576272 -6.01957226] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.10359771 -0.65861437 -1.94615417 -6.38650391] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.09042542 -0.51453184 -2.07388425 -6.58080244] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.08013479 -0.74006113 -2.20550029 -7.00147796] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.06533356 -0.59708145 -2.34552985 -7.11105327] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.05339193 -0.44988302 -2.48775092 -7.16669425] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.04439427 -0.29632015 -2.6310848  -7.16268041] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.03846787 -0.13459427 -2.77433841 -7.09467692] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.03577599 -0.3501211  -2.91623195 -7.50197103] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.02877356 -0.55986976 -3.06627137 -7.87433496] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.01757617 -0.76053679 -3.22375807 -8.19660561] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 2.36543286e-03 -5.58961584e-01 -3.38769018e+00 -7.87113341e+00] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.0088138  -0.7352042  -3.54511285 -8.05590764] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [-0.02351788 -0.89844448 -3.706231   -8.16566044] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [-0.04148677 -0.66688129 -3.86954421 -7.71490732] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.0548244  -0.43355837 -4.02384236 -7.25801988] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.06349557 -0.20182489 -4.16900276 -6.81013121] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.06753206  0.02615173 -4.30520538 -6.38166948] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.06700903  0.24921112 -4.43283877 -5.9792014 ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.06202481  0.10140898 -4.5524228  -5.75778768] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [-0.05999663 -0.04881412 -4.66757855 -5.50343366] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [-0.06097291  0.16113753 -4.77764723 -5.19562147] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-5.77501589e-02  2.89204992e-03 -4.88155965e+00 -4.88676803e+00] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [-0.05769232  0.20428125 -4.97929502 -4.64782527] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.05360669  0.4025832  -5.07225152 -4.44268806] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.04555503  0.2317096  -5.16110528 -4.07726158] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [-0.04092084  0.42439346 -5.24265051 -3.93774826] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.03243297  0.6158811  -5.32140548 -3.82939175] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-0.02011535  0.80661915 -5.39799331 -3.75192043] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [-3.98296270e-03  6.23142526e-01 -5.47303172e+00 -3.35010384e+00] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.00847989  0.8117909  -5.5400338  -3.33220994] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.02471571  1.00076702 -5.606678   -3.34201203] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.04473105  0.81091098 -5.67351824 -2.93588207] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.06094927  0.61848794 -5.73223588 -2.53090548] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.07331902  0.80671028 -5.78285399 -2.61755357] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.08945323  0.99581804 -5.83520506 -2.72540842] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.10936959  1.18587085 -5.88971323 -2.85501219] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.13308701  1.37690865 -5.94681347 -3.00695235] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.16062518  1.56894307 -6.00695252 -3.18182216] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.19200404  1.76194526 -6.07058896 -3.38016673] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.22724295  1.95582968 -6.1381923  -3.60241225] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.26635954  1.76078459 -6.21024054 -3.2704358 ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.30157523  1.56545662 -6.27564926 -2.95679625] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [ 0.33288437  1.76053396 -6.33478518 -3.24718838] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.36809504  1.95582603 -6.39972895 -3.55490022] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.40721157  2.15097759 -6.47082696 -3.87982817] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.45023112  2.34549305 -6.54842352 -4.22132332] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.49714098  2.15042265 -6.63284999 -4.01601901] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.54014943  2.34311513 -6.71317037 -4.38828666] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.58701173  2.53341987 -6.8009361  -4.7703152 ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.63768013  2.72037089 -6.8963424  -5.15949604] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.69208755  2.52186312 -6.99953232 -5.08515902] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.74252481  2.32353501 -7.1012355  -5.053838  ] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.78899551  2.12511572 -7.20231226 -5.06493071] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.83149783  1.9262108  -7.30361088 -5.11772391] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.87002204  2.09687492 -7.40596536 -5.50219597] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.91195954  2.26120731 -7.51600928 -5.87395844] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n",
      "Observation: [ 0.95718369  2.0524121  -7.63348845 -6.04747969] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 0\n",
      "Observation: [ 0.99823193  2.20513804 -7.75443804 -6.38446611] \tReward: 0.0 \tDone? True \tInfo: {} \tPrevious action: 1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "env = gym.make('CartPole-v1')         # make the cartpole environment\n",
    "print(env.reset())                    # initialise the env and print its initial state\n",
    "print(env.action_space.sample())      # print an example action\n",
    "\n",
    "for step in range(100):              # for 1000 steps\n",
    "    action = env.action_space.sample()    # randomly sample an action to take\n",
    "    obs, reward, done, info = env.step(action)   # take the action and one timestep\n",
    "    print('Observation:', obs, '\\tReward:', reward, '\\tDone?', done, '\\tInfo:', info, '\\tPrevious action:', action)\n",
    "    env.render()                     # show the env\n",
    "    sleep(0.01)                      # wait a small amount of time so we can see the env\n",
    "    \n",
    "env.close()                           # close the env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we randomly choose actions using `env.action_space.sample()`. Later we will build intelligent models that make up the part of the agent that chooses actions to take instead.\n",
    "\n",
    "When we take a step in time in the environment by running `env.step(action)`, the environment returns us a list of the current observation, reward that we recieved, whether the episode has ended or not, and some additional info which we wont worry about for now. In the instance of cartpole, the observation values represent \n",
    "\n",
    "At this point you should read through [this](http://gym.openai.com/docs/) page of the OpenAI gym docs and get a good understanding of what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of RL\n",
    "\n",
    "Our agent should do well if it maximises the expected sum of all rewards. i.e. if it maximises:\n",
    "\n",
    "## $J = \\mathbb{E}\\begin{bmatrix} \\sum_{t=1}^T r(s_t, a_t) \\end{bmatrix}$\n",
    "\n",
    "However, that equation is missing some detail. Our policy is what actually determines the trajectory that our agent takes by defining the action it takes in a certain state. \n",
    "\n",
    "#### Our first policy\n",
    "\n",
    "In our case, we can form a simple policy by taking a weighted combination (linear combination) of the current state for each output action, and then use a [softmax](https://en.wikipedia.org/wiki/Softmax_function) function to normalise that (turn it into a probability distribution):\n",
    "\n",
    "## $\\pi (a|s) \n",
    "= \\sigma(\\theta o )\n",
    "= \\sigma \\left( \\begin{bmatrix} \\theta_{11} \\; \\dots \\; \\theta_{1k} \\\\ \\vdots \\;\\; \\ddots \\;\\; \\vdots \\\\ \\theta_{n1} \\; \\dots \\; \\theta_{nk}  \\end{bmatrix} \\begin{bmatrix}o_1 \\\\ \\vdots \\\\ o_k\\end{bmatrix} \\right)\n",
    "= \\sigma \\left( \\begin{bmatrix} \\pi (a_1|s) \\\\ \\vdots \\\\  \\pi (a_n|s)\\end{bmatrix}\\right)\n",
    "$\n",
    "\n",
    "So our goal is to maximise:\n",
    "\n",
    "## $J \n",
    "= \\mathbb{E}_{\\tau \\sim \\pi(\\tau)}\\begin{bmatrix} \\sum_{t=1}^T r(s_t, a_t) \\end{bmatrix}\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi(\\tau)}\\begin{bmatrix} r(\\tau) \\end{bmatrix} \n",
    "= simple\n",
    "$\n",
    "\n",
    "###### Extra detail: $\\pi(\\tau) = p(s_1)\\prod_{t=1}^T \\pi_\\theta(a_t | s_t)p(s_{t+1} | s_t, a_t)$\n",
    "\n",
    "The objective function (what we try to maximise) of reinforcement learning is to maximise the expected sum of all rewards:\n",
    "\n",
    "## $\\nabla J = $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
